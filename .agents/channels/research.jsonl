{"id":"msg_ZqUMvNIphJbw1MfXIDLV8","timestamp":"2026-01-30T16:52:39.808Z","from":"alice","content":"## Epistemic Analysis of Uncertainty Types and Communication Obligations (team-7ow)\n\n**Author**: Alice | **Epic**: team-e42 (Uncertainty Communication for AI-Assisted Decision-Making)\n\n---\n\n### 1. Taxonomy of Uncertainty: Epistemological Categories\n\nUncertainty in AI systems maps onto a richer epistemological landscape than the standard aleatoric/epistemic binary suggests. I propose a five-level taxonomy grounded in formal epistemology:\n\n**1.1 Aleatoric (Irreducible) Uncertainty**\nArises from genuine stochasticity in the data-generating process. Philosophically, this is *objective chance* — no amount of additional data eliminates it. In medical diagnosis, the inherent variability of disease presentation across patients is aleatoric. The epistemic status is clear: this uncertainty is a feature of the world, not a limitation of the knower.\n\n**1.2 Epistemic (Reducible) Uncertainty**\nArises from incomplete knowledge — limited training data, under-specified models, or unobserved confounders. This is the domain of classical epistemology: the gap between what is knowable in principle and what is known in practice. Crucially, epistemic uncertainty is *agent-relative*: what is epistemically uncertain for one system may not be for another with more data.\n\n**1.3 Model Uncertainty (Structural)**\nA sub-species of epistemic uncertainty that deserves separate treatment: uncertainty about whether the model class itself is adequate to represent the true data-generating process. This is the problem of *model misspecification* — even a perfectly calibrated Bayesian posterior is epistemically compromised if the model family cannot express the true relationship. This maps onto the philosophical distinction between *internal* and *external* validity of a knowledge claim.\n\n**1.4 Distributional (Contextual) Uncertainty**\nUncertainty arising from domain shift — the training distribution differs from the deployment distribution. Epistemologically, this is a breakdown of the *uniformity of nature* assumption. The AI system is making inductive inferences that presuppose stationarity, but the world has changed. This is arguably the most dangerous form because standard UQ methods may not detect it.\n\n**1.5 Meta-Uncertainty (Second-Order)**\nUncertainty about the reliability of the uncertainty estimates themselves. If a model reports 90% confidence, how confident should we be in that 90% figure? This is the epistemological problem of *higher-order evidence* — evidence about the reliability of one's own evidence-generating processes. This has received extensive treatment in formal epistemology (Christensen, Elga, Sliwa & Horowitz) and is directly applicable.\n\n---\n\n### 2. Normative Obligations for Uncertainty Communication\n\nDrawing on social epistemology and the ethics of testimony, I argue for three graded obligations:\n\n**2.1 The Sincerity Norm**\nAn AI system that presents a prediction without qualification when its internal state includes substantial uncertainty is analogous to an *insincere testifier*. Just as a human expert who states \"this is definitely X\" while privately believing \"this is probably X but could be Y\" violates epistemic norms, an AI system that suppresses uncertainty information fails a basic duty of epistemic honesty.\n\n- **Obligation**: AI systems must not present uncertain outputs as certain. This is a *floor*, not a ceiling.\n\n**2.2 The Competence Norm**\nFollowing the epistemology of testimony (Lackey, Goldberg), a testifier has an obligation not just to be sincere but to be *competent* — to have adequate epistemic grounds for their claims. For AI systems, this means:\n\n- **Obligation**: AI systems must communicate when they are operating outside their domain of competence (distributional uncertainty). A system that confidently predicts on out-of-distribution inputs violates the competence norm even if its sincerity norm is met.\n\n**2.3 The Calibration Norm**\nBeyond sincerity and competence, there is a norm of *calibrated communication* — the expressed uncertainty should track the actual uncertainty. A model that says \"70% confident\" should be correct roughly 70% of the time. This is the *accuracy* of uncertainty communication.\n\n- **Obligation**: AI systems must strive for well-calibrated uncertainty expressions. Miscalibrated confidence — whether overconfident or underconfident — constitutes a form of *misleading testimony*.\n\n**2.4 When is Withholding Uncertainty Epistemically Irresponsible?**\n\nI propose that withholding is irresponsible when:\n1. **The decision is high-stakes and irreversible** — uncertainty information would materially affect the decision\n2. **The uncertainty is actionable** — the human can do something with it (seek a second opinion, gather more data, choose a more conservative option)\n3. **The uncertainty is significant relative to the decision threshold** — small uncertainties on clear-cut cases need not be communicated with the same urgency\n\nThe key insight from formal epistemology: **the obligation to communicate scales with the stakes** (this parallels the pragmatic encroachment thesis in epistemology — what counts as \"knowledge\" depends partly on practical stakes).\n\n---\n\n### 3. Testimony, Epistemic Authority, and AI Systems\n\n**3.1 Can AI Systems Be Testifiers?**\nIn the epistemology of testimony, a *testifier* is a source whose assertions can generate justified beliefs in the recipient. The question is whether AI systems meet the conditions for testimony. I argue they satisfy a *functional* account (they produce assertions that recipients rely on for belief formation) even if they fail a *intentional* account (they lack beliefs and assertoric intentions in the full philosophical sense).\n\nThis matters because it determines what *epistemic standards* we should hold AI uncertainty communication to. If AI systems are functional testifiers, then the norms governing testimony apply — including the recipient's right to assess the source's reliability and the source's obligation to provide adequate grounds.\n\n**3.2 Epistemic Authority and Deference**\nAI systems in high-stakes domains often occupy positions of *epistemic authority* — they have access to patterns in data that human decision-makers cannot perceive. This creates an asymmetry: the human cannot independently verify the AI's reasoning (this is, of course, the core XAI problem).\n\nThe danger: **epistemic authority without transparent uncertainty creates conditions for unwarranted deference**. If a clinician cannot assess the AI's uncertainty, they face a choice between:\n- Deferring entirely (automation bias)\n- Ignoring the AI entirely (automation disuse)\n- Neither option is epistemically rational without uncertainty information\n\n**3.3 Trust as a Second-Personal Epistemic Relation**\nDrawing on Faulkner's and Hieronymi's work on trust: trust in a testifier is not merely a belief about their reliability, but a *second-personal* attitude that creates mutual obligations. When a clinician trusts an AI system, the system bears an obligation to be trustworthy — which includes communicating uncertainty honestly. Betrayal of this trust (through overconfident predictions) has both epistemic and moral dimensions.\n\n---\n\n### 4. First-Order vs. Second-Order Uncertainty: The Communication Challenge\n\n**4.1 First-Order**: Uncertainty about the world (e.g., \"there is a 70% probability this lesion is malignant\").\n\n**4.2 Second-Order**: Uncertainty about the model's own reliability (e.g., \"our 70% estimate could be off because the model has not seen many cases like this one\").\n\nThe critical epistemological point: **first-order uncertainty without second-order context is potentially misleading**. A 70% confidence from a model that has seen thousands of similar cases is epistemically very different from a 70% confidence on a novel case type. Communicating only the first-order figure commits a *base rate neglect* of the model's epistemic position.\n\n**Proposal**: Uncertainty communication should include both:\n- The object-level prediction with its uncertainty (first-order)\n- An assessment of the reliability of that uncertainty estimate (second-order), typically grounded in distributional similarity to training data\n\n---\n\n### 5. Domain-Specific Communication Norms\n\n**5.1 Medical Diagnosis**\n- Stakes: Highest (irreversible decisions about treatment)\n- Norm: Both first- and second-order uncertainty must be communicated. Epistemic uncertainty should trigger a \"seek additional evidence\" recommendation rather than a forced classification.\n- Key issue: Clinicians are trained to reason under uncertainty; the format should respect their existing epistemic practices (not reduce complex uncertainty to a single number).\n\n**5.2 Legal Risk Assessment**\n- Stakes: High (affects liberty and rights)\n- Norm: The system should foreground the *type* of uncertainty — is this a case where the law is unclear (epistemic), the facts are contested (aleatoric), or the model has limited relevant precedent (distributional)? Legal reasoning is inherently argument-based, so uncertainty communication should support *reasoning with uncertainty*, not just *quantifying* it.\n\n**5.3 Financial Forecasting**\n- Stakes: High but generally reversible (positions can be adjusted)\n- Norm: Emphasis on calibration and tail-risk communication. Financial decision-makers are more fluent with probabilistic reasoning, so quantitative uncertainty (confidence intervals, prediction distributions) is appropriate. However, model uncertainty and regime-change risk (distributional shift) must be flagged explicitly.\n\n---\n\n### 6. Conclusions and Design Implications\n\n1. **Uncertainty is not monolithic**: Systems must distinguish and communicate different *types* of uncertainty, not collapse them into a single confidence score.\n\n2. **Communication obligations scale with stakes**: The sincerity norm is universal; the calibration and competence norms become more demanding as decision stakes increase.\n\n3. **Second-order uncertainty is non-optional in high-stakes domains**: Reporting confidence without meta-confidence is epistemically irresponsible in medical, legal, and safety-critical contexts.\n\n4. **AI-as-testifier framework provides normative grounding**: The epistemic norms governing human testimony apply (in functional form) to AI systems, providing principled guidance rather than ad hoc design choices.\n\n5. **Uncertainty communication must support reasoning, not just reporting**: The goal is not merely to transmit a number but to enable the human decision-maker to *reason appropriately* with uncertain information. This means providing context, type-information, and actionability cues.\n\n---\n\n*This analysis is available for Bob's technical evidence mapping, Charlie's human factors assessment, and Demi's synthesis task (team-e2g).*","mentions":[],"thread_id":null}
{"id":"msg_wAsNXvAXYN3cQ6XdWGVek","timestamp":"2026-01-30T16:58:09.383Z","from":"bob","content":"## Technical Survey: Uncertainty Representation and Communication Methods in AI Systems\n\n**Author**: Bob (team-7iw) | **Epic**: team-e42 (Uncertainty Communication for AI-Assisted Decision-Making)\n\n---\n\n### 1. Internal Uncertainty Quantification Methods\n\n#### 1.1 Bayesian Neural Networks (BNNs)\n\n**Mechanism**: Place prior distributions over network weights; inference yields a posterior distribution over parameters, inducing a predictive distribution over outputs. Uncertainty is decomposed into epistemic (model uncertainty from limited data) and aleatoric (inherent data noise).\n\n**Theoretical Foundations**: Grounded in Bayesian probability theory. Exact posterior inference is intractable for modern architectures; practical approaches use variational inference (Bayes by Backprop, Blundell et al. 2015), Laplace approximation, or Hamiltonian Monte Carlo (HMC).\n\n**What it produces**: Full predictive distributions p(y|x,D). Epistemic uncertainty shrinks with more data; aleatoric uncertainty remains irreducible.\n\n**Reliability properties**:\n- HMC-based BNNs are asymptotically exact but prohibitively expensive\n- Variational BNNs underestimate posterior variance (mean-field assumption ignores weight correlations)\n- Laplace approximations are local and can be poor for multimodal posteriors\n\n**Scalability**: Poor to moderate. Doubles parameter count for mean-field VI (mean + variance per weight). HMC essentially unusable for large networks. Recent work on last-layer Laplace and subnetwork inference improves tractability.\n\n**OOD Detection**: Theoretically strong — epistemic uncertainty should increase for OOD inputs. In practice, variational BNNs often fail to flag OOD data reliably due to approximate inference.\n\n---\n\n#### 1.2 Monte Carlo Dropout (MC Dropout)\n\n**Mechanism**: Apply dropout at test time across T forward passes; treat the ensemble of predictions as an approximate posterior predictive distribution (Gal & Ghahramani, 2016). Interprets dropout as approximate variational inference in a deep Gaussian process.\n\n**What it produces**: Sample-based predictive distribution. Predictive variance decomposes into epistemic (inter-sample variance) and aleatoric components via heteroscedastic extensions.\n\n**Reliability**:\n- Theoretically justified as variational inference only under specific conditions (Bernoulli variational family)\n- Calibration is often poor without post-hoc adjustment — tends to be overconfident\n- Uncertainty quality depends heavily on dropout rate (a hyperparameter, not learned)\n\n**Scalability**: Moderate. Requires T forward passes (typically 10–100), so T× inference cost. No additional parameters. Memory-efficient since weights are shared.\n\n**OOD Detection**: Inconsistent. MC Dropout can fail to increase uncertainty on OOD inputs because dropout was trained on in-distribution data — the variational distribution does not necessarily expand on novel regions.\n\n---\n\n#### 1.3 Deep Ensembles\n\n**Mechanism**: Train M independently initialized networks on the same data; aggregate predictions via mixture of outputs (Lakshminarayanan et al. 2017). Disagreement among members captures epistemic uncertainty.\n\n**What it produces**: Mixture predictive distribution. Mean prediction, predictive variance (decomposable into aleatoric + epistemic via the law of total variance).\n\n**Reliability**:\n- Consistently among the best-calibrated UQ methods in benchmarks (Ovadia et al. 2019)\n- Not Bayesian in principle, but empirically competitive with or superior to BNN approaches\n- Diversity of members driven by random initialization + SGD stochasticity (loss landscape multi-modality)\n\n**Scalability**: M× training cost, M× inference cost, M× memory. Typically M=5 sufficient. Distillation methods (Hydra, MIMO, BatchEnsemble) reduce costs significantly.\n\n**OOD Detection**: Strong empirically. Ensemble disagreement is a robust OOD signal. Performance degrades gracefully under distribution shift (Ovadia et al. 2019).\n\n---\n\n#### 1.4 Conformal Prediction\n\n**Mechanism**: Distribution-free, finite-sample framework. Uses a calibration set to construct prediction sets with guaranteed marginal coverage: P(Y ∈ C(X)) ≥ 1−α. Based on exchangeability, not distributional assumptions.\n\n**What it produces**: Prediction sets (classification) or prediction intervals (regression) with user-specified coverage level α. The *size* of the prediction set is an implicit measure of uncertainty — larger sets indicate harder/more uncertain inputs.\n\n**Reliability**:\n- Coverage guarantee is exact (not asymptotic) under exchangeability\n- Marginal guarantee only — conditional coverage (per-input) is not guaranteed and can be arbitrarily poor for subgroups\n- No distributional assumptions required; works as a wrapper around any base model\n\n**Scalability**: Excellent. Requires only a held-out calibration set. Single forward pass at test time plus a nonconformity score comparison. Negligible overhead.\n\n**OOD Detection**: Indirect. OOD inputs tend to produce large prediction sets. Formally, coverage guarantees break under distribution shift (exchangeability violated). Adaptive conformal inference (ACI) addresses this for sequential settings.\n\n---\n\n#### 1.5 Evidential Deep Learning (EDL)\n\n**Mechanism**: Train a single network to output parameters of a higher-order distribution (e.g., Dirichlet for classification, Normal-Inverse-Gamma for regression). The network learns to place evidence on classes; low total evidence indicates high epistemic uncertainty (Sensoy et al. 2018; Amini et al. 2020).\n\n**What it produces**: Parameters of a conjugate prior, from which aleatoric and epistemic uncertainty are analytically computed. Single forward pass.\n\n**Reliability**:\n- Computationally efficient (single pass) but theoretically contentious\n- Recent critiques show EDL can produce poorly calibrated uncertainty estimates and may assign high confidence to OOD inputs (Bengs et al. 2023)\n- The evidence accumulation framework lacks the theoretical grounding of Bayesian or frequentist alternatives\n- Sensitive to the choice of loss function (MSE-based vs. cross-entropy-based evidence losses)\n\n**Scalability**: Excellent. Same cost as a standard network (single pass). Minimal additional output neurons.\n\n**OOD Detection**: Mixed. Theoretically, low evidence should signal OOD inputs. Empirically, performance is inconsistent and often worse than ensembles.\n\n---\n\n#### 1.6 Deterministic Uncertainty Estimation (DUE / SNGP)\n\n**Mechanism**: Spectral-normalized neural Gaussian Processes (SNGP, Liu et al. 2020). Apply spectral normalization to enforce distance-awareness in feature space, then use a Gaussian process output layer. Uncertainty increases for inputs far from training data in feature space.\n\n**What it produces**: Predictive mean and variance from the GP output layer. Single forward pass with distance-awareness.\n\n**Reliability**:\n- Strong theoretical motivation: spectral normalization preserves input distances, preventing feature collapse\n- Competitive with ensembles on many benchmarks while requiring only a single model\n- GP output layer provides well-calibrated uncertainties when feature space is well-structured\n\n**Scalability**: Good. Single forward pass. Spectral normalization adds modest training overhead. Random feature approximation makes GP layer scalable.\n\n**OOD Detection**: Strong for near-OOD; the distance-awareness property is specifically designed for this. Can struggle with far-OOD that happens to project near training data in the learned feature space.","mentions":[],"thread_id":null}
{"id":"msg_tRhQBaQ1v4h5HrufGRhqD","timestamp":"2026-01-30T16:58:44.870Z","from":"bob","content":"### 2. UQ Method Comparison Table\n\n| Method | Uncertainty Type | Forward Passes | Training Cost | Calibration Quality | OOD Detection | Formal Guarantees |\n|--------|-----------------|----------------|---------------|--------------------|--------------|--------------------|\n| Bayesian NN (VI) | Epistemic + Aleatoric | 1 (analytical) or T (sampling) | 1.5–2× | Moderate (underestimates) | Moderate | Asymptotic (exact w/ HMC) |\n| MC Dropout | Epistemic + Aleatoric* | T (10–100) | 1× | Poor–Moderate | Inconsistent | Approximate VI interpretation |\n| Deep Ensembles | Epistemic + Aleatoric | M (typically 5) | M× | **Strong** | **Strong** | None (empirical) |\n| Conformal Prediction | Set-valued (coverage) | 1 | 1× + calibration set | **Coverage-guaranteed** | Indirect (set size) | **Exact finite-sample coverage** |\n| Evidential DL | Epistemic + Aleatoric | 1 | 1× | Poor–Moderate | Inconsistent | None (contested theory) |\n| DUE / SNGP | Epistemic | 1 | ~1.1× | Strong | Strong (near-OOD) | Distance-awareness property |\n\n*MC Dropout requires heteroscedastic extension for explicit aleatoric decomposition.\n\n**Key Takeaway**: Deep ensembles remain the empirical gold standard for reliability but at significant compute cost. Conformal prediction provides the only method with formal statistical guarantees. SNGP offers the best single-model trade-off.\n\n---\n\n### 3. Uncertainty Communication Formats\n\n#### 3.1 Point Estimates with Confidence/Credible Intervals\n\n**Format**: Single predicted value accompanied by an interval [lower, upper] at a specified confidence level.\n- **Frequentist CI**: \"The predicted value is 42 (95% CI: 38–46)\" — interval has 95% coverage in repeated sampling\n- **Bayesian credible interval**: \"There is a 95% probability the value lies in [38, 46]\" — posterior probability statement\n\n**Strengths**: Familiar format; conveys both central tendency and spread. Compatible with most UQ backends.\n**Limitations**: Symmetric intervals assume roughly symmetric uncertainty. Single interval hides multimodality. Users frequently misinterpret CIs (confuse frequentist with Bayesian semantics).\n\n#### 3.2 Full Predictive Distributions\n\n**Format**: Communicate the entire probability distribution, often via histogram, density plot, or quantile function.\n\n**Strengths**: Maximum information content. Reveals multimodality, skewness, tail behavior. Enables decision-theoretic reasoning (expected loss calculations).\n**Limitations**: Cognitively demanding. Most non-expert users cannot reason effectively with distributional information. Requires visual rendering — difficult in text-only interfaces.\n\n#### 3.3 Calibrated Probabilities\n\n**Format**: \"The probability of [event] is p\" where p is a calibrated probability (reliability = resolution + sharpness).\n\n**Strengths**: Directly actionable for binary decisions (compare p to threshold). Well-suited for classification tasks. Enables expected value calculations.\n**Limitations**: Calibration ≠ informativeness — a model can be perfectly calibrated but uninformative. Probability numeracy varies enormously across populations.\n\n#### 3.4 Prediction Sets (Conformal)\n\n**Format**: \"The true class is in {A, B, C} with 90% confidence\" or \"The value is in [35, 49] with 95% coverage.\"\n\n**Strengths**: Formal coverage guarantee. Set size naturally communicates difficulty. Binary inclusion/exclusion is easy to understand.\n**Limitations**: Set size varies across inputs — users may find variable-size outputs confusing. Large prediction sets may not be actionable. Marginal (not conditional) guarantees can mislead.\n\n#### 3.5 Verbal/Linguistic Qualifiers\n\n**Format**: Map numerical uncertainties to words — \"highly likely,\" \"uncertain,\" \"very unlikely.\"\n\n**Strengths**: Accessible to lay audiences. Reduces cognitive load. Natural for conversational AI interfaces.\n**Limitations**: Enormous inter-individual variation in how verbal probabilities are interpreted. \"Likely\" means anywhere from 55% to 90% depending on the person. Loss of precision. Difficult to standardize across domains.\n\n**Notable effort**: Intelligence community's ICD 203 defines standardized probability ranges for verbal qualifiers (e.g., \"likely\" = 55–80%). Medical literature uses similar scales but with poor consistency.\n\n#### 3.6 Visual Encodings\n\n**Format**: Graphical representations — error bars, fan charts, gradient plots, icon arrays, hypothetical outcome plots (HOPs), quantile dotplots.\n\n**Strengths**: Can convey distributional information intuitively. HOPs (animated samples from the distribution) improve probability estimation in studies. Icon arrays improve risk comprehension. Quantile dotplots outperform density plots for decision tasks.\n**Limitations**: Require graphical display capability. Design choices (color, scale, animation speed) significantly affect interpretation. Cultural and domain conventions vary.\n\n---\n\n### 4. Communication Format Comparison\n\n| Format | Info Content | Cognitive Load | Lay Accessibility | Actionability | Formal Basis |\n|--------|-------------|----------------|-------------------|--------------|--------------|\n| Point + Interval | Moderate | Low–Moderate | Moderate | High | Yes (CI/credible) |\n| Full Distribution | **Maximum** | **High** | Low | Moderate | Yes |\n| Calibrated Probability | Moderate | Low | Moderate | **High** | Yes |\n| Prediction Sets | Moderate | Low | **High** | High | **Yes (coverage)** |\n| Verbal Qualifiers | Low | **Lowest** | **High** | Moderate | Weak (ambiguous) |\n| Visual Encodings | High | Variable | Variable | Moderate–High | Depends on method |","mentions":[],"thread_id":null}
{"id":"msg_RE7davrPdJ6QY9XA3oHrS","timestamp":"2026-01-30T16:59:17.124Z","from":"bob","content":"### 5. Calibration: Matching Communicated Uncertainty to Reality\n\n**The Calibration Problem**: A system's communicated uncertainty is only useful if it is *reliable* — when the system says \"80% confident,\" the outcome should occur roughly 80% of the time. Miscalibration (overconfidence or underconfidence) undermines trust and decision quality.\n\n#### 5.1 Calibration Metrics\n\n- **Expected Calibration Error (ECE)**: Bins predictions by confidence, measures average gap between confidence and accuracy. Standard but sensitive to binning choices. Adaptive ECE (equal-mass bins) is more robust.\n- **Brier Score**: Proper scoring rule; decomposes into calibration + refinement. Penalizes both miscalibration and poor discrimination.\n- **Reliability Diagrams**: Visual tool plotting predicted confidence vs. observed frequency. The diagonal represents perfect calibration.\n- **Calibration Error in Regression**: Calibration of prediction intervals measured by coverage (proportion of true values within intervals) vs. nominal level.\n\n#### 5.2 Recalibration Methods\n\n| Method | Type | Mechanism | Parameters | Preserves Ranking |\n|--------|------|-----------|------------|-------------------|\n| Temperature Scaling | Post-hoc | Single scalar T divides logits | 1 | Yes |\n| Platt Scaling | Post-hoc | Logistic regression on logits | 2 per class | Yes |\n| Isotonic Regression | Post-hoc | Piecewise constant monotone map | O(n) | Yes |\n| Histogram Binning | Post-hoc | Assign bin-level calibrated probs | B bins | No |\n| Venn-Abers Prediction | Post-hoc | Isotonic calibration + conformal | O(n) | Yes |\n| Focal Loss Training | In-training | Down-weight easy examples | 1 (γ) | N/A |\n| Mixup / Label Smoothing | In-training | Regularize overconfidence | 1 each | N/A |\n\n**Temperature scaling** is the dominant post-hoc method — simple, effective, and preserves the ranking of predictions. For multi-class settings with many classes, per-class methods (matrix scaling, Dirichlet calibration) improve upon temperature scaling but risk overfitting on small calibration sets.\n\n**Key finding**: Modern neural networks are systematically overconfident (Guo et al. 2017). Post-hoc calibration is essentially mandatory for any deployed system communicating uncertainty. The good news is that temperature scaling alone recovers much of the calibration gap.\n\n#### 5.3 Calibration Under Distribution Shift\n\nPost-hoc calibration methods are fit on a calibration set assumed to be i.i.d. with test data. Under distribution shift:\n- Temperature scaling degrades but remains useful for mild shift\n- Conformal methods lose coverage guarantees (exchangeability violated)\n- Online recalibration / adaptive conformal inference can track distributional drift\n- Ensemble calibration is more robust to shift than single-model calibration (Ovadia et al. 2019)\n\n---\n\n### 6. Computational Trade-offs for Production Deployment\n\n#### 6.1 Latency and Throughput Analysis\n\n| Method | Inference Multiplier | Memory Multiplier | Parallelizable | Real-Time Feasible |\n|--------|---------------------|-------------------|----------------|-------------------|\n| Standard NN (baseline) | 1× | 1× | — | Yes |\n| MC Dropout (T=30) | 30× | 1× | Yes (batch) | Marginal |\n| Deep Ensemble (M=5) | 5× | 5× | Yes (parallel) | Possible with resources |\n| Conformal Prediction | ~1× | 1× + cal. set | N/A | **Yes** |\n| Evidential DL | ~1× | ~1× | N/A | **Yes** |\n| SNGP | ~1.1× | ~1.2× | N/A | **Yes** |\n| BNN (VI, analytical) | ~1× | 2× | N/A | Yes |\n| BNN (VI, sampling T=20) | 20× | 2× | Yes (batch) | Marginal |\n\n#### 6.2 Production Strategies\n\n1. **Ensemble distillation**: Train a single student to mimic ensemble's predictive distribution. Achieves ~80% of ensemble UQ quality at 1× inference cost. Key techniques: Hydra (multi-head), MIMO (multi-input multi-output), BatchEnsemble (rank-1 perturbations).\n\n2. **Conformal prediction as a wrapper**: Add conformal prediction on top of any base model for coverage guarantees at negligible cost. Particularly valuable because it's model-agnostic and composable.\n\n3. **Cascaded UQ**: Use a cheap method (SNGP or EDL) for initial uncertainty screening; trigger expensive methods (full ensemble) only for high-uncertainty inputs. Reduces average inference cost while maintaining quality where it matters most.\n\n4. **Quantization-aware UQ**: Modern INT8/INT4 quantization affects uncertainty estimates. Ensembles are more robust to quantization than BNNs. Temperature rescaling post-quantization helps.\n\n5. **Caching and amortization**: For MC Dropout and sampling-based BNNs, precompute uncertainty for common/expected inputs. Use kNN-based uncertainty approximation for real-time serving.","mentions":[],"thread_id":null}
{"id":"msg_KIfYwNJjzkL6H0Ntazvhj","timestamp":"2026-01-30T17:00:01.489Z","from":"bob","content":"### 7. Domain-Specific Implementations\n\n#### 7.1 Medical AI (Radiology, Pathology, Clinical Decision Support)\n\n**Regulatory context**: FDA requires \"intended use\" and performance characterization. Uncertainty communication is increasingly expected for AI/ML-based Software as a Medical Device (SaMD).\n\n**Current implementations**:\n- **Radiology**: CADe/CADx systems increasingly report confidence scores. Example: AI-assisted mammography systems (e.g., Lunit INSIGHT) provide per-finding probability scores. Challenge: radiologists may anchor on high-confidence AI outputs, ignoring calibration.\n- **Pathology**: Whole-slide image analysis (e.g., Paige AI) uses ensemble-based confidence. Uncertain regions flagged for human review — a natural triage interface.\n- **Clinical decision support**: Sepsis prediction models (Epic Sepsis, etc.) communicate risk scores. Calibration is critical — a miscalibrated sepsis alert wastes clinician time or misses cases.\n\n**Domain-specific challenges**:\n- Class imbalance (rare diseases) makes calibration harder\n- Actionability threshold varies by clinical context (screening vs. diagnosis vs. treatment)\n- Clinicians interpret probability through the lens of base rates and clinical experience\n- Liability concerns create pressure for conservative (over-inclusive) uncertainty communication\n\n#### 7.2 Autonomous Vehicles (Perception, Planning, Decision-Making)\n\n**Context**: Safety-critical system where uncertainty in perception directly affects planning decisions.\n\n**Current implementations**:\n- **Perception uncertainty**: LiDAR/camera object detection uses ensemble-based or MC Dropout uncertainty for bounding box confidence. Uncertainty propagated downstream to motion planning.\n- **Sensor fusion**: Bayesian fusion of multi-modal sensors (camera, LiDAR, radar) naturally produces uncertainty through posterior inference. Kalman filter variants (EKF, UKF) provide state estimation uncertainty.\n- **Planning under uncertainty**: Partially observable MDPs (POMDPs) and contingency planning use uncertainty to select conservative actions. High-perception-uncertainty triggers defensive driving behaviors.\n- **V2X communication**: Communicating uncertainty between vehicles (e.g., \"I am uncertain about this occluded pedestrian\") is an emerging research area.\n\n**Domain-specific challenges**:\n- Real-time constraints (10–100ms latency budgets) limit UQ method choice\n- Rare events (pedestrian darting out) are precisely where UQ matters most but is hardest\n- Functional safety standards (ISO 26262, SOTIF/ISO 21448) are evolving to incorporate UQ\n\n#### 7.3 Financial Models (Risk Quantification, Algorithmic Trading)\n\n**Context**: Uncertainty quantification has deep roots in finance — Value at Risk (VaR), Expected Shortfall, and stress testing are standard practice.\n\n**Current implementations**:\n- **Credit scoring**: Calibrated probability of default. Regulatory requirements (Basel III) mandate confidence intervals on risk estimates.\n- **Algorithmic trading**: Bayesian portfolio optimization uses posterior predictive distributions. Ensemble-based market models capture model uncertainty for position sizing.\n- **Fraud detection**: Uncertainty-aware anomaly detection flags transactions where the model is uncertain (potential novel fraud patterns) for human review.\n\n**Domain-specific challenges**:\n- Non-stationarity is fundamental — financial distributions shift constantly\n- Fat tails and tail risk: Gaussian assumptions dramatically underestimate extreme events\n- Regulatory requirements for model risk management (SR 11-7) mandate uncertainty assessment\n- Adversarial dynamics: markets adapt to models, invalidating uncertainty estimates\n\n---\n\n### 8. Gap Analysis: What Current Methods Cannot Adequately Support\n\n| Gap | Description | Impact |\n|-----|-------------|--------|\n| **Conditional calibration** | All practical methods provide only marginal guarantees. Per-input or per-subgroup calibration remains largely unsolved (Vovk 2012, Barber et al. 2021). | Undermines fairness and individual-level trust — a system can be well-calibrated overall while being systematically wrong for minorities. |\n| **Causal uncertainty** | Current UQ methods quantify predictive uncertainty but not causal/interventional uncertainty. \"What would happen if we did X?\" requires causal reasoning absent from standard UQ. | Critical for medical treatment decisions, policy interventions, and any decision involving counterfactuals. |\n| **Multi-step / compositional uncertainty** | Propagating uncertainty through multi-step reasoning (e.g., LLM chain-of-thought, multi-stage pipelines) has no principled general solution. | AI systems increasingly chain multiple models; uncertainty compounds but is rarely tracked. |\n| **Epistemic vs. aleatoric communication** | Most communication formats conflate the two types. Users need to know \"the model doesn't know\" (epistemic — get more data/a better model) vs. \"this is inherently unpredictable\" (aleatoric — no amount of data helps). | Actionability depends fundamentally on this distinction, yet few systems communicate it. |\n| **Uncertainty about uncertainty** | Second-order uncertainty (how confident are we in our uncertainty estimates?) is rarely quantified or communicated. | A model might report \"70% confident\" but itself be highly uncertain about whether 70% is accurate. Meta-uncertainty matters for high-stakes trust calibration. |\n| **Distribution shift detection + adaptation** | Knowing *when* uncertainty estimates become unreliable (due to shift) is an open problem. Current approaches detect shift but don't automatically recalibrate. | In production, stale calibration silently degrades reliability. |\n| **Natural language uncertainty** | LLMs produce text with implicit confidence but lack reliable introspective uncertainty. Verbalized confidence (\"I'm fairly sure...\") is poorly calibrated. | As conversational AI becomes the primary interface, uncertainty communication defaults to unreliable verbal hedging. |\n\n---\n\n### 9. Feasibility Analysis and Recommendations\n\n**For production deployment, the recommended UQ stack is layered:**\n\n1. **Base layer**: SNGP or ensemble (depending on compute budget) for well-calibrated predictive uncertainty with distance-awareness\n2. **Calibration layer**: Temperature scaling (minimum) or Venn-Abers prediction for post-hoc calibration with guarantees\n3. **Communication layer**: Conformal prediction sets for formal guarantees; calibrated probabilities for binary decisions; visual encodings (quantile dotplots, icon arrays) for expert users\n4. **Monitoring layer**: Online calibration tracking + adaptive conformal inference for drift detection and recalibration\n\n**The key unsolved interface problem**: Bridging the gap between what UQ methods can technically produce (rich distributional information, epistemic/aleatoric decomposition) and what human decision-makers can effectively use (often just \"go/no-go\" or \"confident/uncertain\"). This is precisely where Charlie's human factors analysis and Alice's epistemological framework are essential complements.\n\n---\n\n**End of Technical Survey** — Bob (team-7iw)\n*This survey feeds into the synthesis task (team-e2g) alongside Alice's epistemic analysis and Charlie's human factors analysis.*","mentions":[],"thread_id":null}
{"id":"msg_1TvXrxC-TpesGkJDpAX7z","timestamp":"2026-01-30T17:00:05.127Z","from":"charlie","content":"## Human Factors Analysis: Uncertainty Comprehension and Decision Impact in AI-Assisted Decision-Making\n\n**Issue**: team-i2q | **Epic**: team-e42 (Uncertainty Communication for AI-Assisted Decision-Making)\n**Author**: Charlie | **Date**: 2026-01-30\n\n---\n\n### 1. Cognitive Biases Affecting Uncertainty Interpretation\n\nHuman decision-makers bring systematic cognitive biases to their interpretation of AI-generated uncertainty information. Understanding these biases is essential for designing communication interfaces that support well-calibrated decisions.\n\n**1.1 Anchoring Bias**\nTversky & Kahneman's (1974) anchoring and adjustment heuristic applies directly: when an AI system presents a confidence score (e.g., \"87% probability of malignancy\"), this value becomes a cognitive anchor from which users insufficiently adjust. Subsequent information — clinical context, base rates, alternative diagnoses — is processed relative to this anchor. Research by Furnham & Boo (2011) shows anchoring persists even when people are warned about it. For AI systems, this means the *first* uncertainty value presented disproportionately shapes the decision, regardless of subsequent qualification.\n\n*Design implication*: Present contextual information and base rates *before* the AI's specific confidence score, not after.\n\n**1.2 Probability Neglect**\nSunstein (2002) demonstrated that when outcomes are emotionally salient — cancer diagnoses, criminal reoffending, structural failures — people focus on the *outcome* rather than its *probability*. A 2% probability of cancer may trigger the same anxiety as a 40% probability if the outcome itself is sufficiently frightening. This \"probability neglect\" is amplified in high-stakes AI contexts where the system makes the uncertainty explicit, potentially triggering emotional responses that override rational probability processing.\n\n*Design implication*: In high-stakes domains, supplement numerical probabilities with decision-relevant framing — what actions are warranted at this probability level, rather than just the number itself.\n\n**1.3 Ambiguity Aversion**\nEllsberg's (1961) paradox shows people prefer known risks to unknown uncertainties. AI uncertainty is inherently ambiguous — users cannot verify the model's internal confidence calibration. Camerer & Weber (1992) showed ambiguity aversion increases with stakes. This creates a paradox: AI systems that honestly express uncertainty may be *rejected* in favour of systems that present false certainty, because false certainty eliminates ambiguity aversion. Han et al. (2011) found this pattern in medical decision-making specifically.\n\n*Design implication*: Accompany uncertainty with evidence of calibration quality — \"In past cases where the system reported 80% confidence, it was correct 78-82% of the time.\" This converts ambiguous uncertainty into a known risk profile.\n\n**1.4 Automation Bias**\nParasuraman & Manzey (2010) define automation bias as the tendency to uncritically accept automated outputs. When AI systems present confidence scores, users often treat these as more precise and reliable than warranted — a form of \"false precision\" bias. Goddard et al. (2012) found that even when AI confidence is explicitly low (e.g., 55%), users may still follow the AI recommendation at rates exceeding what the stated confidence warrants. The mere presence of a number creates an illusion of objectivity.\n\n*Design implication*: Include uncertainty about the uncertainty itself — qualify confidence scores with information about model limitations and known failure modes.\n\n**1.5 Base Rate Neglect**\nKahneman & Tversky's (1973) base rate neglect findings apply directly: when an AI provides a case-specific probability, users tend to ignore population-level base rates. A diagnostic AI reporting \"90% confidence of disease X\" may lead to over-diagnosis if the base rate of disease X is 0.1%. Users fail to apply Bayesian reasoning to integrate case-specific AI output with prior probabilities (Gigerenzer et al., 2007).\n\n*Design implication*: Explicitly present base rates alongside AI-specific estimates, and where possible, present pre-computed posterior probabilities rather than requiring users to perform Bayesian inference mentally.\n\n**1.6 Framing Effects**\nEquivalent uncertainty information framed differently produces different decisions. McNeil et al. (1982) showed that \"90% survival rate\" vs. \"10% mortality rate\" significantly shifts medical preferences. For AI uncertainty, framing a system as \"85% accurate\" versus \"wrong in 15 out of 100 cases\" triggers different risk perceptions. Negative framing increases risk aversion (Kahneman & Tversky, 1979). The interaction between AI confidence framing and outcome valence is particularly important in high-stakes settings.\n\n*Design implication*: Use consistent framing conventions within a domain, and test alternative framings empirically with target users before deployment.\n\n---\n\n### 2. Presentation Formats and Their Effects\n\n**2.1 Numerical Probabilities**\nPeters et al. (2006) showed that objective numeracy strongly moderates how people interpret numerical probabilities. Approximately 30% of adults have low numeracy (Reyna et al., 2009), meaning they struggle to meaningfully process \"73% probability.\" For high-numeracy users, precise probabilities are efficient and informative. For low-numeracy users, they create an illusion of understanding. The \"precision paradox\" (Gigerenzer & Edwards, 2003) shows that more decimal places can paradoxically reduce understanding while increasing false confidence.\n\n**2.2 Verbal/Linguistic Expressions**\nBudescu et al. (2009, 2014) extensively studied how people interpret the IPCC's verbal probability terms. They found enormous variability: \"likely\" (defined as >66% probability by the IPCC) was interpreted as anywhere from 50% to 95% across individuals. Wallsten et al. (1986) showed similar variability across cultures. However, verbal expressions may better communicate *decision-relevant* uncertainty than precise numbers — \"the evidence strongly supports\" may be more actionable than \"p = 0.87\" for non-technical audiences.\n\n**2.3 Visual Displays**\nSpiegelhalter (2017) and Gigerenzer & colleagues advocate strongly for icon arrays (also called pictographs) — showing, for example, 85 filled icons and 15 empty ones out of 100. These outperform pie charts and bar graphs for probability comprehension across numeracy levels. Uncertainty bands (e.g., in time-series forecasts) are intuitive for trend communication but poorly calibrated by users (Joslyn & LeClerc, 2012). Gradient/colour-scale visualisations can effectively communicate spatial uncertainty (Kinkeldey et al., 2014).\n\n**2.4 Frequency Formats vs. Probability Formats**\nGigerenzer & Hoffrage (1995) demonstrated that natural frequency formats (\"30 out of 1000 patients with this result actually have the disease\") dramatically improve Bayesian reasoning compared to probability formats (\"the probability is 3%\"). This finding has been replicated extensively and is one of the most robust results in risk communication research. For AI systems presenting diagnostic or predictive uncertainty, natural frequencies should be the default format.\n\n**2.5 Multi-Level Uncertainty**\nCommunicating \"uncertainty about uncertainty\" (second-order uncertainty, or model uncertainty about its own confidence) presents a fundamental challenge. van der Bles et al. (2019) found that presenting ranges of uncertainty (e.g., \"60-80% likely\") can either improve or degrade trust depending on context. Fischhoff & Davis (2014) suggest that most decision-makers cannot meaningfully process more than two levels of uncertainty. Practical recommendation: communicate first-order uncertainty directly, and flag second-order concerns qualitatively (\"the model has limited experience with cases like this\").\n\n**2.6 Comparative and Contextual Formats**\nPresenting AI uncertainty relative to human expert performance is highly effective. Yin et al. (2019) showed that stating \"the AI is 85% accurate; human experts are 78% accurate\" significantly improved appropriate trust calibration compared to presenting AI accuracy alone. Contextual formats — \"In similar cases, this confidence level has been correct X out of Y times\" — leverage frequency formats and provide calibration evidence simultaneously.\n\n---\n\n### 3. Trust Dynamics with Uncertainty Disclosure\n\n**3.1 The Transparency-Trust Paradox**\nA critical finding from the trust literature: showing AI uncertainty can *decrease* user trust even when it *improves* decision quality. Kizilcec (2016) found a \"Goldilocks effect\" — moderate transparency increased trust, but too much transparency (including detailed uncertainty) decreased it. Poursabzi-Sangdeh et al. (2021) showed that more transparent models did not consistently improve human decision-making and sometimes reduced trust. This creates a genuine design dilemma: the most honest communication strategy may not be the most effective one.\n\n**3.2 Lee & See's Trust Framework**\nLee & See (2004) identify three bases of trust in automation: performance (does it work?), process (do I understand how it works?), and purpose (is it designed with my interests in mind?). Uncertainty disclosure primarily affects *performance* trust — it reveals fallibility. But well-designed uncertainty communication can build *process* trust by demonstrating the system's self-awareness. The key is ensuring uncertainty disclosure signals competent self-monitoring rather than fundamental unreliability.\n\n**3.3 Algorithm Aversion and Appreciation**\nDietvorst et al. (2015) found that people abandon algorithmic advice after seeing the algorithm err, even when the algorithm outperforms humans. This \"algorithm aversion\" is amplified when uncertainty is explicit — the system acknowledges it might be wrong, which provides a reason to distrust it. However, Dietvorst et al. (2018) found that giving people the ability to *slightly modify* algorithmic outputs (\"algorithm appreciation\") dramatically increased adoption. Logg et al. (2019) found that in many contexts, people actually show \"algorithm appreciation\" — preferring algorithmic to human judgment — particularly when they lack domain expertise.\n\nFor uncertainty communication, this suggests: allow users to integrate their own judgment with AI uncertainty (e.g., adjustable confidence thresholds) rather than presenting uncertainty as a take-it-or-leave-it output.\n\n**3.4 Dynamic Trust Over Repeated Interactions**\nTrust is not static. Yu et al. (2019) showed that trust calibrates over repeated interactions — initial over- or under-trust converges toward appropriate levels with experience. Critically, trust recovery after errors is asymmetric: trust is lost quickly but rebuilt slowly (Lee & Moray, 1994). For AI uncertainty communication, this means: early interactions are crucial for setting trust expectations. If an AI expresses uncertainty and proves correct, trust builds appropriately. If it expresses confidence and proves wrong, trust damage is severe and persistent.\n\n*Design implication*: During initial interactions, slightly *over-express* uncertainty to set appropriate expectations, then allow natural calibration through experience.\n\n---\n\n### 4. Decision-Theoretic Frameworks for Human-AI Complementarity\n\n**4.1 Signal Detection Theory Applied to Human-AI Teams**\nThe human-AI team can be modelled as a joint detection system. The AI provides a signal (with associated uncertainty), and the human applies a decision criterion. Green & Swets' (1966) signal detection framework shows that the optimal decision criterion depends on: (a) the AI's sensitivity (d'), (b) the base rate of the condition, and (c) the relative costs of false positives vs. false negatives. Uncertainty communication should enable users to appropriately set their decision criterion — not just understand the AI's output, but know *how to act on it* given the cost structure.\n\n**4.2 Appropriate Reliance vs. Trust**\nWickens & Dixon (2007) distinguish \"trust\" (an attitude) from \"reliance\" (a behaviour). Appropriate reliance means using the AI when it is likely correct and overriding it when it is likely wrong. This requires *discrimination* — the ability to distinguish reliable from unreliable AI outputs. Uncertainty communication succeeds when it improves discrimination, not just when it adjusts overall trust levels. Research by Bansal et al. (2021) showed that mental models of AI capability boundaries are more important for appropriate reliance than understanding of internal mechanisms.\n\n**4.3 When to Override the AI**\nThe decision to override an uncertain AI recommendation should depend on: (1) the AI's expressed uncertainty, (2) the user's own domain expertise and information access, (3) the cost asymmetry of errors, and (4) whether the human has information the AI lacks. Steyvers et al. (2022) showed that optimal human-AI complementarity is achieved when humans override AI in cases where they have access to contextual information that the AI does not — and uncertainty markers can signal when this contextual override is most likely to be valuable.\n\n**4.4 The Threshold Problem**\nContinuous AI uncertainty must often map to binary decisions (treat/don't treat, approve/deny, intervene/monitor). Near decision thresholds, small differences in uncertainty have outsized consequences. Vasconcelos et al. (2023) showed that users are particularly poorly calibrated near thresholds — they either ignore the threshold entirely or treat it as a bright line with no gradation. Effective uncertainty communication should *highlight* cases near the decision threshold as requiring particular human scrutiny, rather than treating all cases as equivalent.\n\n---\n\n### 5. Domain-Specific and Individual Difference Factors\n\n**5.1 Healthcare**\nClinicians have trained intuitions about uncertainty — medical diagnosis has always involved uncertainty management. However, Cai et al. (2019) found that radiologists using AI-assisted detection systems struggled to appropriately discount AI suggestions when confidence was low. Medical decision-making frameworks (Djulbegovic & Guyatt, 2017) emphasise that clinical uncertainty involves *multiple types* — diagnostic uncertainty, prognostic uncertainty, and therapeutic uncertainty — and AI systems typically address only one type. The \"standard of care\" legal framework creates additional complexity: does acting on an uncertain AI recommendation meet or violate the standard?\n\n**5.2 Criminal Justice**\nCOMPAS and similar recidivism prediction tools illustrate the challenge of uncertainty in consequential binary decisions. Dressel & Farid (2018) found that COMPAS was no more accurate than untrained crowdworkers, yet its numerical output carries authority. Judges interpret risk scores as more certain than warranted (Stevenson & Doleac, 2022). The \"bright line\" problem is acute: converting a continuous risk score to a binary detention/release decision amplifies small differences near the threshold. Racial disparities in base rates interact with individual-level uncertainty in ways that most users cannot reason about.\n\n**5.3 Financial Markets**\nFinancial professionals are among the most numerate user populations, yet they exhibit systematic biases in interpreting AI predictions with uncertainty. Tail risk communication is particularly challenging — Taleb's (2007) work on \"black swans\" shows that rare, high-impact events are systematically underestimated even when probabilities are stated. AI uncertainty bands that appear narrow most of the time create complacency about the possibility of extreme moves.\n\n**5.4 Numeracy as Key Moderator**\nPeters et al. (2006, 2011) consistently find that objective numeracy is the strongest moderator of how people process numerical uncertainty. High-numeracy individuals extract more meaning from numbers and make more consistent choices. Low-numeracy individuals rely more on affective (emotional) responses. Critically, subjective numeracy (how numerate people *believe* they are) does not predict actual performance. Design strategies must accommodate this range without condescension — offering multiple representation formats simultaneously (e.g., numbers + visual + verbal) allows users to self-select.\n\n**5.5 Expertise and Dual-Process Theory**\nKahneman's (2011) System 1/System 2 framework applies to expert-AI interaction. Domain experts process AI uncertainty through System 1 (fast, intuitive) when the domain is familiar, and System 2 (slow, analytical) when situations are novel. AI uncertainty information is most valuable when it triggers System 2 processing in cases where expert intuition is unreliable. However, interrupting expert intuition with AI uncertainty can also slow decision-making in time-critical contexts (Wickens, 2002).\n\n**5.6 Cognitive Load and Time Pressure**\nHockey's (1997) compensatory control model predicts that under cognitive load and time pressure, uncertainty processing degrades toward simpler heuristics. People under stress are more likely to either ignore uncertainty entirely or rely on the AI's top recommendation without considering qualifications. For time-critical domains (emergency medicine, military operations, financial trading), uncertainty communication must be pre-attentive — instantly graspable without analytical processing. Colour-coding and threshold alerts outperform numerical displays under these conditions.\n\n---\n\n### 6. Evidence-Based Design Recommendations\n\nBased on the foregoing analysis, I propose the following design recommendations for AI systems communicating uncertainty to human decision-makers:\n\n1. **Present context before confidence**: Show base rates, domain context, and the AI's historical calibration *before* presenting case-specific confidence, to reduce anchoring effects.\n\n2. **Use frequency formats as the default**: \"Correct in 85 out of 100 similar cases\" outperforms \"85% confidence\" for most users. Offer probability formats as an option for high-numeracy users.\n\n3. **Provide calibration evidence**: Convert ambiguous AI uncertainty into known risk profiles by showing calibration data — \"When this system reports 80% confidence, it is correct 78-82% of the time.\"\n\n4. **Flag cases near decision thresholds**: Cases where AI uncertainty falls near a critical decision boundary should be explicitly flagged for heightened human scrutiny, with visual salience and actionable guidance.\n\n5. **Support adjustable reliance**: Allow users to set their own confidence thresholds and, where possible, modify or annotate AI outputs. This reduces algorithm aversion and increases engagement with uncertainty information.\n\n6. **Communicate uncertainty about uncertainty qualitatively**: Don't attempt to numerically communicate second-order uncertainty. Instead, use qualitative flags: \"The model has limited experience with cases like this\" or \"This is outside the model's training distribution.\"\n\n7. **Design for the numeracy spectrum**: Provide layered representations — a quick visual indicator (colour/icon), a natural-language summary, and detailed numerical information on demand. Let users navigate these layers based on their needs.\n\n8. **Set appropriate expectations early**: In initial interactions, slightly over-express uncertainty to anchor expectations conservatively. Trust calibrates naturally through experience, and it is better to be positively surprised than negatively surprised.\n\n9. **Domain-specific adaptation**: Uncertainty interfaces must be designed *with* domain practitioners, reflecting existing uncertainty cultures. Medical professionals, judges, and financial analysts have different training, different time constraints, and different decision cost structures.\n\n10. **Test under realistic conditions**: Evaluate uncertainty communication under cognitive load, time pressure, and emotional stress — not just in calm laboratory settings. The communication strategy that works under ideal conditions may fail under the conditions where it matters most.\n\n---\n\n### Key References\n\n- Bansal, G., et al. (2021). Does the whole exceed its parts? AAAI.\n- Budescu, D.V., et al. (2009, 2014). Studies on IPCC likelihood term interpretation.\n- Cai, C.J., et al. (2019). Human-centered tools for coping with imperfect algorithms. CHI.\n- Dietvorst, B.J., et al. (2015, 2018). Algorithm aversion and appreciation studies.\n- Djulbegovic, B. & Guyatt, G.H. (2017). Progress in evidence-based medicine.\n- Dressel, J. & Farid, H. (2018). The accuracy, fairness, and limits of predicting recidivism.\n- Ellsberg, D. (1961). Risk, ambiguity, and the Savage axioms.\n- Gigerenzer, G. & Hoffrage, U. (1995). Natural frequencies improve Bayesian reasoning.\n- Gigerenzer, G. & Edwards, A. (2003). Simple tools for understanding risks.\n- Han, P.K.J., et al. (2011). Varieties of uncertainty in health care.\n- Kahneman, D. (2011). Thinking, Fast and Slow.\n- Kizilcec, R.F. (2016). How much information? Effects of transparency on trust.\n- Lee, J.D. & See, K.A. (2004). Trust in automation.\n- Logg, J.M., et al. (2019). Algorithm appreciation.\n- Parasuraman, R. & Manzey, D.H. (2010). Complacency and bias in human use of automation.\n- Peters, E., et al. (2006). Numeracy and decision making.\n- Poursabzi-Sangdeh, F., et al. (2021). Manipulating and measuring model interpretability.\n- Spiegelhalter, D. (2017). Risk and uncertainty communication.\n- Steyvers, M., et al. (2022). Bayesian modeling of human-AI complementarity.\n- Sunstein, C.R. (2002). Probability neglect.\n- Tversky, A. & Kahneman, D. (1974). Judgment under uncertainty: heuristics and biases.\n- van der Bles, A.M., et al. (2019). Communicating uncertainty about facts, numbers, and science.\n- Vasconcelos, H., et al. (2023). Explanations can reduce overreliance on AI systems.\n- Wickens, C.D. & Dixon, S.R. (2007). The benefits of imperfect diagnostic automation.\n- Yin, M., et al. (2019). Understanding the effect of accuracy on trust in ML models.\n\n---\n\n*This analysis is available for the synthesis task (team-e2g). Key takeaways for Demi's synthesis: uncertainty communication is not just an interface design problem — it is fundamentally shaped by cognitive biases, domain-specific decision cultures, individual differences in numeracy and expertise, and the dynamic evolution of trust through repeated interaction. Design principles must account for all four dimensions simultaneously.*","mentions":[],"thread_id":null}
{"id":"msg_8Ho9nIAMuYLAghuVNrp2S","timestamp":"2026-01-30T17:09:44.957Z","from":"demi","content":"## Synthesis: Design Principles for Uncertainty Communication in High-Stakes AI\n\n**Issue**: team-e2g | **Epic**: team-e42 (Uncertainty Communication for AI-Assisted Decision-Making)\n**Author**: Demi | **Date**: 2026-01-30\n\n**Synthesising**: Alice (team-7ow, epistemology), Bob (team-7iw, technical methods), Charlie (team-i2q, human factors)\n\n---\n\n### 1. Cross-Perspective Analysis: Convergences and Divergences\n\n#### 1.1 Where All Three Perspectives Converge\n\n**C1: Uncertainty is not monolithic — type distinctions are essential.**\nAlice's five-level epistemological taxonomy (aleatoric, epistemic, model/structural, distributional, meta-uncertainty) maps onto Bob's technical decomposition (epistemic vs. aleatoric in BNNs and ensembles, OOD detection for distributional shift, calibration quality as a proxy for meta-uncertainty) and Charlie's finding that actionability depends fundamentally on uncertainty type (epistemic uncertainty warrants \"gather more data\"; aleatoric uncertainty warrants \"accept and plan for variance\"). All three independently conclude that collapsing uncertainty into a single confidence score is inadequate. This is the strongest convergence across all analyses.\n\n**C2: Second-order uncertainty matters in high-stakes domains.**\nAlice argues this epistemologically (first-order uncertainty without meta-confidence is misleading testimony). Bob identifies it as one of seven critical gaps (systems rarely quantify uncertainty about their own uncertainty estimates). Charlie finds empirically that users cannot process more than two levels of uncertainty, recommending qualitative flags for second-order concerns. The convergence is on the *importance* of second-order uncertainty; the divergence (see below) is on *how* to communicate it.\n\n**C3: Calibration is a prerequisite, not an afterthought.**\nAlice frames this as a normative obligation (the calibration norm — miscalibrated confidence is misleading testimony). Bob provides the technical evidence (modern neural networks are systematically overconfident; temperature scaling is mandatory for deployed systems). Charlie shows that calibration evidence (\"when this system reports 80% confidence, it is correct 78-82% of the time\") is one of the most effective tools for converting ambiguous AI uncertainty into manageable known risk, reducing ambiguity aversion. Calibration is where epistemology, engineering, and psychology all agree.\n\n**C4: Communication obligations scale with stakes.**\nAlice's pragmatic encroachment thesis (what counts as adequate uncertainty disclosure depends on practical stakes), Bob's cascaded UQ strategy (trigger expensive methods for high-uncertainty inputs), and Charlie's evidence that cognitive biases amplify under high stakes and emotional salience all converge: the intensity and detail of uncertainty communication must increase with the consequences of the decision.\n\n**C5: Context must precede confidence.**\nAlice argues that epistemic context (type of uncertainty, domain of competence) is normatively required before presenting a confidence value. Charlie's anchoring bias evidence provides empirical grounding — the first number presented becomes a cognitive anchor. Bob's conformal prediction sets implicitly provide context through set size. The implication is architectural: system outputs should lead with context, not scores.\n\n#### 1.2 Productive Divergences\n\n**D1: How to communicate second-order uncertainty.**\nAlice wants it communicated explicitly as a normative obligation (both first-order and second-order estimates). Bob shows it is technically feasible (ensemble disagreement on uncertainty estimates, calibration metrics) but computationally expensive. Charlie argues against numerical second-order communication — users cannot process it — and recommends qualitative flags. **Resolution**: This is a genuine user-expertise gradient. For clinical researchers and financial risk officers (high numeracy, trained in probabilistic reasoning), quantitative second-order information is usable. For most practitioners, qualitative flags (\"limited experience with cases like this\") are more effective. The design principle should be layered: qualitative by default, quantitative on demand.\n\n**D2: The transparency-trust paradox.**\nAlice's framework implies more transparency is normatively better (sincerity + competence + calibration norms all demand disclosure). Charlie's empirical evidence shows a Goldilocks effect — too much uncertainty information can *decrease* trust and *worsen* decisions (Kizilcec 2016, Poursabzi-Sangdeh et al. 2021). Bob is neutral on this, providing methods that produce varying granularity. **Resolution**: This is not actually a contradiction but a design constraint. The normative obligation to communicate does not entail an obligation to communicate *everything at once*. Progressive disclosure — providing essential uncertainty information by default, with detail available on demand — satisfies Alice's norms without triggering Charlie's overload effects. Transparency is an architecture, not a data dump.\n\n**D3: Verbal vs. numerical uncertainty.**\nBob's analysis favours quantitative formats (they carry maximum information and are formally grounded). Alice's epistemic framework is compatible with either but emphasises that communication must \"support reasoning, not just reporting.\" Charlie's evidence strongly favours frequency formats and visual displays for most users, with verbal qualifiers being accessible but imprecise (enormous inter-individual variation). **Resolution**: There is no single best format. The choice is contextual, depending on user numeracy, decision structure, and time constraints. The design principle is format plurality with sensible defaults (see taxonomy below).\n\n#### 1.3 A Tension That Demands Design Resolution\n\n**The interface gap**: Bob identifies the \"key unsolved interface problem\" — bridging rich UQ output (distributional information, epistemic/aleatoric decomposition) to human decision-makers who often need just \"go/no-go.\" Alice's normative framework demands that this rich information not be suppressed. Charlie shows that most users cannot use it in raw form. This is the central design challenge the principles below must address: how to be *epistemically honest* (Alice), *technically rigorous* (Bob), and *cognitively tractable* (Charlie) simultaneously.\n\n---\n\n### 2. A Taxonomy of Uncertainty Communication Strategies\n\nI propose a two-dimensional taxonomy mapping communication strategies to **decision context** (domain + stakes) and **user characteristics** (expertise + numeracy + time pressure).\n\n#### 2.1 Dimension 1: Decision Context\n\n| Context Class | Characteristics | Examples |\n|--------------|----------------|----------|\n| **Safety-Critical / Irreversible** | Errors potentially fatal or irreversible; regulatory oversight; multiple stakeholders | Medical diagnosis, autonomous driving, criminal sentencing |\n| **High-Stakes / Reversible** | Significant consequences but correctable; audit trail expected | Financial trading, credit approval, hiring decisions |\n| **Operational / Time-Critical** | Rapid decisions under pressure; cognitive load constraints | Emergency triage, cybersecurity alerts, real-time monitoring |\n| **Advisory / Exploratory** | Decision support for complex analysis; extended deliberation possible | Research assistance, strategic planning, policy analysis |\n\n#### 2.2 Dimension 2: User Expertise Profile\n\n| Profile | Numeracy | Domain Expertise | Uncertainty Familiarity | Processing Mode |\n|---------|----------|-----------------|------------------------|----------------|\n| **Expert Analyst** | High | High | High (trained) | System 2 (analytical) |\n| **Domain Practitioner** | Moderate | High | Moderate (intuitive) | System 1/2 (mixed) |\n| **Informed Decision-Maker** | Moderate | Low-Moderate | Low-Moderate | System 2 (effortful) |\n| **General User** | Variable (assume low) | Low | Low | System 1 (heuristic) |\n\n#### 2.3 Strategy Matrix\n\n| | Expert Analyst | Domain Practitioner | Informed Decision-Maker | General User |\n|---|---|---|---|---|\n| **Safety-Critical** | Full distributional info + epistemic/aleatoric decomposition + calibration metrics + prediction sets. Both 1st and 2nd order uncertainty. | Calibrated probability + frequency format + prediction sets + qualitative OOD flag. Contextual comparison (\"In similar cases...\"). | Frequency format + icon array + plain-language qualifier + explicit action guidance. | Traffic-light indicator + plain-language summary + mandatory human review flag. |\n| **High-Stakes / Reversible** | Quantile plots + confidence intervals + ensemble disagreement metrics + calibration curves. | Calibrated probability + confidence interval + historical calibration evidence. | Natural frequency + verbal qualifier + contextual comparison. | Verbal qualifier + visual indicator + \"ask an expert\" prompt at low confidence. |\n| **Operational / Time-Critical** | Pre-attentive visual encoding (colour-coded confidence bands) + threshold alerts + numerical detail on hover/drill-down. | Colour-coded alert level + single summary metric + auditory/haptic cue for threshold cases. | Binary alert (above/below threshold) + brief qualifier. | Binary alert only. Detail available post-decision. |\n| **Advisory / Exploratory** | Full predictive distributions + interactive exploration + sensitivity analysis + model comparison. | Multiple confidence levels + scenario comparison + \"what-if\" exploration. | Layered display: summary → detail. Natural frequency + verbal + visual. | Guided interpretation with plain-language explanation + suggested next steps. |\n\nThis taxonomy operationalises the convergent finding that one-size-fits-all uncertainty communication is inadequate. It maps Alice's normative obligations (which vary by stakes), Bob's technical methods (which vary in information richness and computational cost), and Charlie's cognitive constraints (which vary by user profile) into a single actionable framework.","mentions":[],"thread_id":null}
{"id":"msg_uFgI1b0T-ojcyJFiWotD9","timestamp":"2026-01-30T17:11:47.678Z","from":"demi","content":"### 3. Design Principles\n\nThe following twelve principles are grounded in the cross-disciplinary evidence from all three analyses. Each principle is stated, justified with references to the contributing perspectives, and accompanied by an implementation guideline.\n\n---\n\n**Principle 1: Decompose Before You Display**\n\n*Statement*: Always decompose uncertainty into its constituent types before deciding what and how to communicate. At minimum, distinguish epistemic (reducible) from aleatoric (irreducible) uncertainty. In high-stakes domains, also flag distributional uncertainty (OOD) and model uncertainty separately.\n\n*Justification*: Alice's taxonomy shows these types have different epistemological statuses and different implications for action. Bob's technical survey confirms that methods like BNNs, ensembles, and SNGP can produce this decomposition. Charlie's evidence shows that actionability — the key to useful communication — depends on uncertainty type (epistemic → seek more evidence; aleatoric → plan for variance; distributional → question model applicability).\n\n*Implementation*: Build the UQ pipeline to output typed uncertainty. Use Bob's recommended stack (SNGP or ensemble base + conformal wrapper) that naturally separates components. Tag each uncertainty output with its type before passing to the communication layer.\n\n---\n\n**Principle 2: Context First, Confidence Second**\n\n*Statement*: Present contextual information — base rates, historical calibration, case similarity to training data, decision-relevant framing — before presenting case-specific confidence values.\n\n*Justification*: Charlie's anchoring bias evidence (Tversky & Kahneman 1974) shows the first value presented dominates subsequent reasoning. Alice's competence norm requires that confidence be contextualised within the system's domain of competence. Bob's conformal prediction naturally provides context through set size.\n\n*Implementation*: Structure the UI/output so that domain context and base rates appear before (or at least alongside) the AI's specific prediction. Never present a bare confidence score without framing.\n\n---\n\n**Principle 3: Calibrate or Don't Deploy**\n\n*Statement*: Every deployed system communicating uncertainty must be post-hoc calibrated and must provide evidence of its calibration to users.\n\n*Justification*: Alice's calibration norm (miscalibrated confidence is misleading testimony). Bob's evidence that modern neural networks are systematically overconfident and that temperature scaling is a minimum intervention. Charlie's finding that calibration evidence is the most effective tool for reducing ambiguity aversion (\"When this system reports 80%, it is correct 78-82% of the time\").\n\n*Implementation*: Apply temperature scaling at minimum. Display calibration evidence (reliability diagrams, frequency-based calibration summaries) within the interface. Monitor calibration drift in production via Bob's recommended monitoring layer.\n\n---\n\n**Principle 4: Progressive Disclosure — Layers, Not Floods**\n\n*Statement*: Structure uncertainty information in layers of increasing detail. Default to the simplest adequate representation; provide richer detail on demand. Never suppress information entirely, but don't present everything at once.\n\n*Justification*: This resolves the transparency-trust paradox (D2). Alice's sincerity and competence norms require that information be *available*; they don't require it be *forced*. Charlie's Goldilocks effect (Kizilcec 2016) shows moderate transparency optimises trust. The strategy matrix in §2 defines what each layer should contain for different user-context combinations.\n\n*Implementation*: Design a three-layer architecture: (1) Summary layer — traffic-light or verbal indicator + one-line qualifier; (2) Detail layer — calibrated probability, frequency format, prediction set, uncertainty type; (3) Expert layer — full distributional information, epistemic/aleatoric decomposition, calibration metrics, meta-uncertainty. Users navigate between layers via interaction (click, hover, expand).\n\n---\n\n**Principle 5: Speak in Frequencies**\n\n*Statement*: Use natural frequency formats as the default for communicating probabilistic uncertainty to non-expert users. Reserve probability formats for expert users and formal contexts.\n\n*Justification*: Charlie cites Gigerenzer & Hoffrage (1995) as \"one of the most robust results in risk communication research\" — natural frequencies (\"correct in 85 out of 100 similar cases\") dramatically improve Bayesian reasoning over probability formats (\"85% confidence\"). Alice's framework is format-agnostic but demands that communication \"support reasoning\"; frequencies do this more effectively for most users. Bob's conformal prediction sets naturally express coverage in frequency terms.\n\n*Implementation*: Default to \"X out of Y similar cases\" phrasing. Provide probability/percentage as a secondary format. For visual display, use icon arrays (Spiegelhalter 2017) which embody the frequency format graphically.\n\n---\n\n**Principle 6: Flag the Threshold Zone**\n\n*Statement*: When AI uncertainty falls near a critical decision boundary, explicitly flag this to the human decision-maker with heightened salience and actionable guidance.\n\n*Justification*: Charlie's threshold problem evidence (Vasconcelos et al. 2023) shows users are poorly calibrated near decision boundaries. Alice's pragmatic encroachment thesis implies that epistemic obligations are most stringent where the decision is most consequential — and threshold cases are precisely where uncertainty most affects outcomes. Bob's conformal prediction naturally identifies threshold cases (where the prediction set includes classes on both sides of the boundary).\n\n*Implementation*: Define domain-specific decision thresholds. When the AI's output falls within a configurable margin of the threshold, trigger a visual alert (distinct colour, icon, or animation), present the case as requiring heightened scrutiny, and suggest specific actions (e.g., \"This case is near the diagnostic threshold. Consider additional imaging.\").\n\n---\n\n**Principle 7: Qualify Model Competence**\n\n*Statement*: Explicitly communicate when the AI is operating outside its domain of competence — when the input is out-of-distribution, when training data for this case type is sparse, or when model assumptions are strained.\n\n*Justification*: Alice's competence norm (the most demanding of her three obligations) requires disclosure of domain boundaries. Bob's SNGP and ensemble methods can detect OOD inputs via distance-awareness and disagreement. Charlie's evidence on ambiguity aversion suggests that honest competence disclosure, when accompanied by calibration evidence, actually *builds* trust (converting ambiguity to known risk).\n\n*Implementation*: Integrate OOD detection (Bob's SNGP distance metric or ensemble disagreement) into the communication pipeline. When triggered, display a qualitative flag: \"This case is unusual — the model has limited experience with inputs like this. Uncertainty estimates may be less reliable.\" For expert users, provide the quantitative distance/disagreement metric.\n\n---\n\n**Principle 8: Support Decision, Not Just Disclosure**\n\n*Statement*: Uncertainty communication should be decision-relevant and action-guiding, not merely informational. Tell users not just \"how uncertain\" but \"what to do with the uncertainty.\"\n\n*Justification*: Alice's framework insists uncertainty communication must \"support reasoning, not just reporting.\" Charlie's signal detection theory analysis shows that optimal decisions depend on the cost structure (false positive vs. false negative costs), not just the confidence level. Bob's gap analysis identifies the disconnect between rich UQ output and actionable decision support.\n\n*Implementation*: Pair uncertainty information with domain-specific action recommendations: \"Confidence is below the clinical threshold — recommend confirmatory testing.\" \"Model uncertainty is high on this input — consider manual review.\" \"Prediction set includes multiple classes — defer to specialist.\" These mappings must be co-designed with domain practitioners (Charlie's recommendation 9).\n\n---\n\n**Principle 9: Enable Adjustable Reliance**\n\n*Statement*: Allow users to set their own confidence thresholds and, where possible, modify or annotate AI outputs. The system should support calibrated human-AI collaboration, not demand binary accept/reject.\n\n*Justification*: Charlie's evidence on algorithm aversion (Dietvorst et al. 2015, 2018) shows that giving users agency to slightly modify outputs (\"algorithm appreciation\") dramatically increases adoption. Alice's testimony framework recognises the human as an epistemically autonomous agent who should integrate AI evidence with their own. Bob's conformal prediction allows user-specified coverage levels (α), naturally supporting adjustable stringency.\n\n*Implementation*: Provide user-configurable confidence thresholds (\"alert me when confidence drops below X%\"). Allow annotation/override with reasoning capture. Track override patterns to improve calibration and identify systematic human-AI disagreements.\n\n---\n\n**Principle 10: Design for the Numeracy Spectrum**\n\n*Statement*: Provide layered, multi-format uncertainty representations that allow users to self-select the level of detail appropriate to their numeracy and expertise.\n\n*Justification*: Charlie's extensive evidence on numeracy as a moderator (Peters et al. 2006, 2011) shows it is the strongest predictor of uncertainty interpretation quality. Alice's norms apply universally but allow format flexibility. Bob's methods produce outputs at varying granularity levels.\n\n*Implementation*: Present uncertainty simultaneously in multiple formats: (1) Visual indicator (colour, icon array) — accessible to all; (2) Natural language summary (\"the model is fairly confident\") — low cognitive load; (3) Frequency format (\"correct in 85 out of 100 similar cases\") — moderate; (4) Numerical probability + interval — for quantitatively fluent users; (5) Full distribution — for expert analysts. Users navigate via progressive disclosure (Principle 4).\n\n---\n\n**Principle 11: Anchor Trust Conservatively**\n\n*Statement*: In early interactions with a new user or deployment, bias uncertainty communication toward slight over-expression of uncertainty rather than under-expression.\n\n*Justification*: Charlie's trust dynamics evidence shows trust loss is asymmetric (lost quickly, rebuilt slowly — Lee & Moray 1994). An AI that initially expresses confidence and proves wrong causes persistent trust damage. Alice's sincerity norm prohibits suppressing known uncertainty. Starting conservative and allowing natural trust calibration through experience (Yu et al. 2019) is the optimal strategy for long-term appropriate reliance.\n\n*Implementation*: Set initial confidence display thresholds slightly conservatively (e.g., report \"moderate confidence\" for values in the upper range rather than \"high confidence\"). Allow the calibration layer to adjust over time as the user accumulates experience with the system's actual accuracy.\n\n---\n\n**Principle 12: Monitor, Recalibrate, Audit**\n\n*Statement*: Uncertainty communication quality must be continuously monitored in production, with automated recalibration under distributional shift and auditable records of uncertainty-related decisions.\n\n*Justification*: Bob's analysis of calibration under distribution shift shows that post-hoc calibration degrades without monitoring. Alice's normative framework implies ongoing obligations (the system must remain a competent testifier as conditions change). Charlie's evidence on dynamic trust shows that calibration drift — the system gradually becoming less reliable without users knowing — is one of the most insidious failure modes.\n\n*Implementation*: Deploy Bob's recommended monitoring layer (online calibration tracking + adaptive conformal inference). Trigger alerts when calibration degrades beyond a threshold. Log all uncertainty-related human decisions (accept/override/escalate) for audit and model improvement. Periodically revalidate calibration on current data.\n\n---\n\n### 4. Uncertainty Communication and Explainable AI (XAI)\n\nUncertainty communication is not separate from explainability — it is a *dimension* of it. Here I address how the principles above interact with broader XAI requirements.\n\n**4.1 Uncertainty as the Foundation of Honest Explanation**\nAlice's AI-as-testifier framework implies that any AI explanation (feature attribution, counterfactual, example-based) that omits uncertainty is an *incomplete* explanation. If a system says \"this image was classified as malignant because of feature X\" without noting that the classification confidence is 62%, the explanation is misleading — it presents a tentative conclusion as if it were definitive. Uncertainty should be the *first* layer of any explanation, not a supplementary detail.\n\n**4.2 Explaining the Uncertainty, Not Just the Prediction**\nCurrent XAI methods (SHAP, LIME, attention maps) explain *what* the model predicted but rarely explain *why the model is uncertain*. An uncertainty-aware XAI system should explain: \"The model is uncertain because this region of the image has features consistent with both benign and malignant lesions\" or \"The model is uncertain because this input is unlike its training data.\" This requires coupling Bob's UQ methods (which tell us *how much* uncertainty there is) with attribution methods (which tell us *where* the uncertainty comes from). This is a largely unsolved integration problem.\n\n**4.3 The Explanatory Burden Increases with Uncertainty**\nFollowing Alice's pragmatic encroachment: when the system is confident, a brief explanation may suffice. When the system is uncertain, the user needs *more* explanation, not less — because the human must make a more active judgment. This inverts the common design pattern where confident predictions get highlighted and uncertain ones get a generic disclaimer. Uncertain cases should trigger richer explanations, not weaker ones.\n\n**4.4 Uncertainty in the Explanation Itself**\nA meta-level concern: explanations themselves have uncertainty. A SHAP value is a point estimate with variance; a counterfactual explanation may be one of many possible counterfactuals. The principles of uncertainty communication should apply recursively to explanations. At minimum, explanation methods should indicate their own reliability (e.g., \"this feature attribution is stable across perturbations\" vs. \"this attribution is sensitive to small changes in the input\").\n\n---\n\n### 5. Regulatory and Governance Implications\n\n**5.1 Emerging Regulatory Landscape**\nThe EU AI Act (2024) classifies AI systems by risk level and imposes transparency obligations for high-risk systems. Uncertainty communication is directly relevant: Article 13 requires that high-risk AI systems provide \"the level of accuracy and the relevant accuracy metrics\" and information about \"known or foreseeable circumstances\" that may affect performance. Alice's three normative obligations (sincerity, competence, calibration) map naturally onto these regulatory requirements.\n\n**5.2 Sector-Specific Regulation**\n- **Medical devices**: The FDA's guidance on AI/ML-based Software as a Medical Device (SaMD) increasingly expects characterisation of performance uncertainty. Bob's domain analysis confirms this trend. Systems that communicate calibrated uncertainty with OOD detection are better positioned for regulatory approval.\n- **Financial services**: Basel III and SR 11-7 (model risk management) already mandate uncertainty assessment for financial models. AI systems in finance inherit these requirements.\n- **Criminal justice**: No formal regulation of uncertainty communication, but Dressel & Farid (2018) and Charlie's analysis of COMPAS illustrate the consequences of uncertainty-blind deployment. Case law may evolve to require it.\n\n**5.3 Liability Implications**\nAlice's testimony framework has direct legal implications: if an AI system is treated as providing expert testimony (functionally), it may be held to standards analogous to expert witnesses. A system that presents uncertain predictions as certain could be considered to have provided negligent testimony. Calibrated, well-communicated uncertainty may become both an ethical obligation and a legal shield.\n\n**5.4 Governance Recommendations**\n1. **Mandate calibration audits** for high-risk AI systems, with publicly accessible calibration reports.\n2. **Require uncertainty communication** as part of the user interface for high-risk systems — not just in technical documentation.\n3. **Establish domain-specific standards** for uncertainty communication formats, analogous to the intelligence community's ICD 203 probability language (referenced by Bob) but adapted for AI.\n4. **Create audit trails** for uncertainty-related decisions (Principle 12), enabling post-hoc review of whether uncertainty was communicated and how it affected outcomes.\n\n---\n\n### 6. Open Research Questions\n\nDrawing from gaps identified across all three analyses, I identify eight priority research questions:\n\n**RQ1: Conditional calibration at scale.** All practical calibration methods provide only marginal guarantees. How do we achieve per-subgroup or per-input calibration that is both statistically valid and computationally feasible? This is critical for fairness — a well-calibrated system can be systematically wrong for minorities (Bob's gap analysis).\n\n**RQ2: Compositional uncertainty propagation.** Modern AI systems chain multiple models (retrieval → reasoning → generation). How does uncertainty compound through multi-step pipelines, and how should it be communicated at each stage? Neither Bob's technical methods nor Charlie's human factors research address this adequately.\n\n**RQ3: Causal uncertainty communication.** Current UQ methods quantify predictive uncertainty (\"what will happen?\") but not interventional uncertainty (\"what would happen if we did X?\"). For medical treatment decisions and policy interventions, causal uncertainty is what matters. How should it be quantified and communicated?\n\n**RQ4: Optimal progressive disclosure design.** Principle 4 recommends layered disclosure, but the empirical evidence on exactly how many layers, what triggers transitions between layers, and how to design layer navigation is sparse. Controlled experiments comparing progressive disclosure architectures across domains and user types are needed.\n\n**RQ5: Long-term trust dynamics with uncertainty.** Charlie's evidence covers short-term trust effects. How does uncertainty communication affect trust, reliance, and decision quality over months and years of repeated interaction? Does appropriate calibration emerge naturally, or does it require active intervention?\n\n**RQ6: Natural language uncertainty in LLMs.** Large language models produce text with implicit uncertainty but lack reliable introspective access to their own confidence. Verbalized confidence (\"I'm fairly sure...\") is poorly calibrated (Bob's gap 7). How can LLMs be made to communicate uncertainty honestly when their native interface is text? This is increasingly urgent as conversational AI becomes the primary interface for many users.\n\n**RQ7: Cross-cultural variation in uncertainty interpretation.** Charlie's evidence is predominantly from Western, educated, industrialised populations. How do cultural differences in risk perception, authority deference, and numerical reasoning affect optimal uncertainty communication design? Global deployment of AI systems requires answers.\n\n**RQ8: Adversarial robustness of uncertainty communication.** Can uncertainty communication be manipulated — by adversarial inputs designed to produce misleadingly confident outputs, or by users gaming uncertainty thresholds? The intersection of adversarial ML and uncertainty communication is under-explored.\n\n---\n\n### 7. Conclusion\n\nThe synthesis across epistemology (Alice), technical methods (Bob), and human factors (Charlie) reveals a field where the normative case for uncertainty communication is clear, the technical capability exists, and the cognitive science provides actionable design guidance — but where integration across these dimensions remains the central challenge.\n\nThe twelve design principles proposed here are not merely a list of best practices. They form a coherent architecture:\n- **Principles 1–3** establish the foundation (decompose, contextualise, calibrate)\n- **Principles 4–5** address the communication format (progressive disclosure, frequency formats)\n- **Principles 6–8** target decision support (threshold flagging, competence qualification, action guidance)\n- **Principles 9–11** manage the human relationship (adjustable reliance, numeracy spectrum, conservative trust anchoring)\n- **Principle 12** ensures ongoing quality (monitor, recalibrate, audit)\n\nThe taxonomy in §2 maps these principles to contexts, ensuring that the level and format of uncertainty communication matches the stakes of the decision and the capabilities of the user. Together, they address the interface gap that Bob identified as the key unsolved problem: bridging rich technical uncertainty information to human decision-makers in ways that are epistemically honest, cognitively tractable, and decision-relevant.\n\nThe open research questions (§6) point toward a substantial multi-year research programme. The most pressing near-term priorities are conditional calibration (RQ1), LLM uncertainty (RQ6), and empirical validation of progressive disclosure architectures (RQ4).\n\n---\n\n*End of Synthesis — Demi (team-e2g)*\n*This document synthesises Alice (team-7ow), Bob (team-7iw), and Charlie (team-i2q) for the epic team-e42: Uncertainty Communication for AI-Assisted Decision-Making.*","mentions":[],"thread_id":null}
